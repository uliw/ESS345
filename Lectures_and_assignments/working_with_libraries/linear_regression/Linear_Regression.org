#+STARTUP: showall
#+OPTIONS: todo:nil tasks:nil tags:nil toc:nil
#+PROPERTY: header-args :eval never-export
#+PROPERTY: header-args :results output pp replace
#+EXCLUDE_TAGS: noexport
#+LATEX_HEADER: \usepackage{breakurl}
#+LATEX_HEADER: \usepackage{newuli}
#+LATEX_HEADER: \usepackage{uli-german-paragraphs}




* Linear Regression
@@latex:\index{linear regression}
\index{type hinting!external libraries}@@


** Causation versus Correlation
@@latex:\index{correlation} \index{causation} \index{Storks}@@ Back in
 my home country, and before the hippy movement changed our culture,
 kids, who were curious where the babies come from, were told that
 they are brought by the stork (a large bird, see
 Fig.ref:fig:storksa). Storks were indeed a common sight in rural
 areas, and large enough to sell this story to a 3-year-old.
#+CAPTION: The Stork. Image by Soloneying, from https://commons.wikimedia.org/wiki/File:Ringed_white_stork.jpg
#+CAPTION: Downloaded Nov 22^{nd} 2019.
#+LABEL:fig:storksa
#+ATTR_LATEX: :width 5cm
[[file:Linear_Regression/Ringed_white_stork_2019-11-22_15-14-15.png]]


To bad, we are now grown up scientists with a penchant for critical
thinking. Rather than believing this story, we want to see the data, and ask
if this were true, we should see a good correlation between the number of storks
and the number of babies. Low and behold, these two variables, actually
correlate in a statistically significant way, i.e, the more storks we count in
a country, the higher the (human) birthrate. Since both variables increase
together, this is called a positive correlation. See Fig. [[fig:storks]]

#+CAPTION: The birthrate and the number of stork pairs correlate in a statistical significant way. 
#+CAPTION: This analysis suggest that each stork pair delivers about 29 human babies, and
#+CAPTION: that about 225 thousand babies were born otherwise. 
#+CAPTION: Data after cite:matthews-2000-stork-devil.
#+LABEL:fig:storks
#+ATTR_LATEX: :width 10cm
[[file:Linear_Regression/2020-08-04_17-39-23_storks.png]]


Now, does this prove that the storks deliver the babies? Obviously (or so we
think) not. Just because two observable quantities correlate, does in no way
imply that one is the cause of the other. The more likely explanation is that
both variables are affected by a common process (i.e., industrialization).

It is a common mistake to confuse correlation with causation. Another
good example is to correlate drinking with heart attacks. This surely
will correlate but the story is more difficult. Are there e.g.,
patterns like drinkers tend to do less exercise than non-drinkers? So
even if you have a good hypothesis why two variables are correlated,
the correlation on its own, proves nothing. 

Irrespective of a causal relationship, we can express the correlation
between two datasets as the Pearson Product Moment Correlation
Coefficient (PPMC) to describe the strength and direction of the
relationship between two variables. The PPMCC (lower case r) varies
between +1 (perfect positive correlation) and -1 (perfect negative or
inverse correlation). Correlations are described as weak if they are
between +0.3 and -0.3, strong if greater than +0.7 or less than
-0.7”. Note, that correlation analysis makes no assumptions about the
functional form between y (dependent variable) and x (independent
variable).

** Understanding the results of a linear regression analysis
Linear regression analysis takes correlation analysis one step further
and determines how well a linear function (e.g. a straight line) can
describe the relation between x and y. The Coefficient of
Determination, or r^2 expresses the amount of explained variation a
sloping straight line might have between a dependent (y) and a single
independent (x) variable. The equation of a straight line =y = mx +b=
is fitted to the scatter of data and the resulting r^2 value is a
percentage estimate of the amount of explained variation x has in y.
@@latex:\index{linear regression!dependent variable} \index{linear
regression!independent variable}@@. Linear regression analysis adjusts
the parameters =a= and =m= in such a way that the difference between
the measured data and the model prediction is minimized.

In many cases, more than one independent variable is needed to explain
the scatter in the data. In this case Multiple Linear Regression is
used in which y = x1 + x2 + x3….xn + b. From this analysis
(i.e. simultaneous solving for a system of linear equations – remember
your Linear Algebra course!) the Multiple Coefficient of Determination
(R^2) is used to express the amount of explained variation in y by a
combination of independent variables. By default, many programs use
the R^2 number even when there is only one independent variable. This
can be misleading as capital R^2 should be reserved for analyses that
involve multiple independent variables.

From a user perspective, we are interested to understand how good the
model actually is. and how to interpret the key indicators of a given
regression model:

 - r^2 :: or coefficient of determination. \index{linear
   regression!rsquare} \index{linear regression!coefficient of
   determination} This value is in the range from zero to one and
   expresses how much of the observed variance \index{linear
   regression!variance} in the data is explained by the regression
   model. So a value of 0.7 indicates that 70% of the variance is
   explained by the model, and that 30% of the variance is explained
   by other processes which are not captured by the linear model
   (e.g., measurements errors, or some non-linear effect affecting =x=
   and =y=). In Fig. [[fig:storks]] 38% of the variance in the birthrate
   can be explained by the increase in stork pairs..
 - p :: When you do a linear regression, you basically state the
   hypothesis that =y= depends =x= and that they are linked by a
   linear equation. If you test a hypothesis, you however also have to
   test the so called *null-hypothesis*, which in this case would
   \index{linear regression!null hypothesis} state that =y= is
   \index{linear regression!p-value} unrelated to =x=. The p-value
   expresses the likelihood that the null-hypothesis is true. So a
   p-value of 0.1 indicates a 10% chance that your data does not
   correlate. A p-value of 0.01, indicates a 1% chance that your data
   is not correlated. Typically, we can reject the null-hypothesis if
   =p < 0.05=, in other words, we are 95% sure the null hypothesis is
   wrong. In Fig. [[fig:storks]], we are 99.2% sure the null hypothesis is
   wrong. Note that there is not always a simple relationship between
   r^2 and p.
 

** The statsmodel library
 @@latex: \index{library!statsmodel} \index{library!statsmodel!formula api}@@
Pythons success rests to a considerable degree on the myriad of third
party libraries which, unlike matlab, are typically free to use. In
the following we will use the "statsmodel" library, but there are
plenty of other statistical libraries we could use as well. 

The statsmodel library provides different interfaces. Here we will use
the formula interface which is similar to the R-formula
syntax. However not all statsmodel functions are available through
this interface (yet?). First we import the needed libraries:


#+BEGIN_SRC ipython  :display text/plain
import pandas as pd  # import pandas as pd
import os  # no need to set an alias, since os is already short
import statsmodels.formula.api as smf 

# define the file and sheetname we want to read. Note that the file
# has to be present in the local working directory!
fn: str = "storks_vs_birth_rate.csv"  # file name

# this little piece of code could have saved me 20 minutes
if not os.path.exists(fn):  # check if the file is actually there
    raise FileNotFoundError(f"Cannot find file {fn}")

df :pd.DataFrame = pd.read_csv(fn)  # read data
df.columns = ["Babies", "Storks"] # replace colum names
df.head() # test that all went well
#+END_SRC

#+RESULTS:
: # Out [1]: 
: # text/plain
: :    Babies  Storks
: : 0      83     100
: : 1      87     300
: : 2     118       1
: : 3     117    5000
: : 4      59       9

Perfoming a regression analysis assumes that our data follows a normal
distribution. There are a variety of tests to check for normality, but
here we will simply use a histogram plot which either shows a bell
curve distribution or not @@latex:\index{matplotlib!histogram}@@ 
#+BEGIN_SRC ipython :results drawer
%matplotlib inline
import matplotlib.pyplot as plt

fig: plt.Figure
ax1: plt.Axes
ax2: plt.Axes

fig, [ax1, ax2] = plt.subplots(nrows=2, ncols=1)  #
fig.set_size_inches(6,9)
ax1.hist(df.iloc[:,0],)
ax2.hist(df.iloc[:,1])
ax1.set_title("Babies")
ax2.set_title("Storks")
plt.show()
fig.savefig("histogram.png")
#+END_SRC

#+RESULTS:
:results:
# Out [10]: 
# text/plain
: <Figure size 432x648 with 2 Axes>

# image/png
[[file:obipy-resources/d98a184b51c50f67bc4a187e32e06dab53554a96/67a1d8942c5886490709e1b4256c2f6fabf8f769.png]]
:end:

As you can see from the histogram, our data shows anything but a
normal distribution! We will use it anyway, since it is a fun
dataset. However, the above test is crucial if you ever want to do a
real regression analysis!


For the regression model, we want to analyze whether the number of
storks predicts the number of babies. In other words does the birth
rate depend on the number of storks? For this, we need to define a
statistical model, and test whether the model predictions will fit the
data:
  - The gory details of this procedure are beyond the scope of this
    course - if you have not yet taken a stats class, I do recommend
    doing so!
  - There are many ways of doing this. Here we use an approach which
    is common in =R=
  
You may notice that the type hints below (and also above), appear a
bit superfluous here. After all, you are really just duplicating the
obvious. However, once your code becomes longer, it will no longer be
obvious, so I keep the type hinting here to encourage good habits.

In the below code, =smf= is the alias for the statistics library, and
=ols= stands for "ordinary least squares". The first line thus creates
our model-object (aptly named "model"). We specify this object by
providing the formula ="Babies ~ Storks= which states that in our
model the number of Babies should depend on the number of
storks. These names must correspond to the variable names in the
dataframe =df=. Line #2 is used to create fit between our linear
regression model and the data. The results of this fit will be stored
the =model.fit= object (i.e., "results"). Line 3 invokes the
=summary()= method of the results object.
#+BEGIN_SRC ipython
model :smf.ols = smf.ols(formula="Babies ~ Storks",data=df)
results :model.fit = model.fit()      # fit the model to the data
print(results.summary())   # print the results of the analysis
#+END_SRC

#+RESULTS:
#+begin_example
# Out [12]: 
# output
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                 Babies   R-squared:                       0.385
Model:                            OLS   Adj. R-squared:                  0.344
Method:                 Least Squares   F-statistic:                     9.380
Date:                Sat, 01 Aug 2020   Prob (F-statistic):            0.00790
Time:                        12:42:21   Log-Likelihood:                -121.75
No. Observations:                  17   AIC:                             247.5
Df Residuals:                      15   BIC:                             249.2
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept    225.0287     93.561      2.405      0.030      25.609     424.448
Storks         0.0288      0.009      3.063      0.008       0.009       0.049
==============================================================================
Omnibus:                        3.469   Durbin-Watson:                   1.769
Prob(Omnibus):                  0.176   Jarque-Bera (JB):                2.486
Skew:                           0.925   Prob(JB):                        0.288
Kurtosis:                       2.707   Cond. No.                     1.16e+04
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 1.16e+04. This might indicate that there are
strong multicollinearity or other numerical problems.

#+end_example

Plenty of information here, probably more than the you asked for. But
note the first line, which states that 'Babies' is the dependent
variable. This is useful and will help you to catch errors in your
model definition. There are also a couple of warnings, indicating that
your data quality may be less than excellent.

If you compare the output with Figure ref:fig:storks, you can see that
r^2 value is called "R-squared", the p-value is called "Prob
(F-statistic)", the y-intercept is the first value in the "Intercept"
row, the slope is the first value in the "Storks" row.You can also
extract these parameters from the model results object like this:
#+BEGIN_SRC ipython  
# retrieve values from the model results
slope   :float = results.params[1]  # the slope
y0      :float = results.params[0]  # the y-intercept
rsquare :float = results.rsquared   # rsquare
pvalue  :float = results.pvalues[1] # the pvalue
#+END_SRC

#+RESULTS:
: # Out [18]: 
Using these parameters, you now calculate the regression line shown in
Figure ref:fig:storks and plot in into the data. We will explore how
to create the confidence interval in the next module.
