#+STARTUP: showall
#+OPTIONS: todo:nil tasks:nil tags:nil toc:nil
#+PROPERTY: header-args :eval never-export
#+PROPERTY: header-args :results output pp replace
#+EXCLUDE_TAGS: noexport
#+LATEX_HEADER: \usepackage{breakurl}
#+LATEX_HEADER: \usepackage{newuli}
#+LATEX_HEADER: \usepackage{uli-german-paragraphs}




* Linear Regression
\index{linear regression}
\index{type hinting!external libraries}
Before diving into linear regression analysis, I need to elaborate a
bit on type hinting variable types which are defined by external
libraries. Since type hinting is a rather new addition to python, most
third party packages have no support for it. On the other hand, it is
a useful tool, which reminds us that while =ax.scatter()= and
=sns.scatterplot()= look very similar, =ax= is a plot handle, whereas
=sns= is an alias to the seaborne library.

The typing library provides support to declare generic variable types,
and we used this without much explanation before, e.g.: \index{type
hinting!TypeVar()}
#+BEGIN_EXAMPLE
pdf = TypeVar('pandas.core.frame.DataFrame')
#+END_EXAMPLE
however, how would you know the argument to =TypeVar()=? We can simply
declare a data-frame object, and then query its type:
#+BEGIN_SRC ipython
import pandas as pd
newDF = pd.DataFrame() #create empty dataframe
print(type(newDF))     #print its type
#+END_SRC

#+RESULTS:
: # Out [10]: 
: # output
: <class 'pandas.core.frame.DataFrame'>
: 

so now you know that pandas dataframe is of type
='pandas.core.frame.DataFrame=, and we can rewrite the above code as
#+BEGIN_SRC ipython
from typing import TypeVar
import pandas as pd

pdf = TypeVar('pandas.core.frame.DataFrame')
newDF :pdf = pd.DataFrame() #create empty dataframe
#+END_SRC

#+RESULTS:
: # Out [11]: 

Granted, line 5 above is pretty redundant, however, sometimes we don't
want to create the actual data object in the header section of our
program, consider e.g., the case of a plot handle which is specific to
a given figure handle. You may not want to declare these in the top
section of your program, but it would still be useful to declare all
the types we use for type hinting. Python allows us to do this with
sort of a non statement. We can, e.g., write:
#+BEGIN_SRC ipython
a : int # this is a counter
#+END_SRC

#+RESULTS:
: # Out [12]: 

So we define the type of a variable, but we do not initialize its
value (add a =print(a) to test for yourself). This allows us to
explain all variables at the beginning of our code, without actually
initializing them. In the following, please use the following type
hint definitions in your code:
#+BEGIN_SRC ipython
# declare the non standart typ hints
pdf = TypeVar('pandas.core.frame.DataFrame')
pds = TypeVar('pandas.core.series.Series')
smm = TypeVar('statsmodels.regression.linear_model.OLS')
smr = TypeVar('statsmodels.regression.linear_model.RegressionResultsWrapper')
npa = TypeVar('numpy.ndarray')
#+END_SRC

#+RESULTS:
: # Out [13]: 



** Correlation versus Correlation
\index{Correlation} \index{Causation} \index{Storks}
 Back in my home country, and
before the hippy movement changed our culture, kids, who were curious
where the babies come from, were told that they are brought by the
stork (a large bird, see Fig.ref:fig:storksa). Storks were indeed a
common sight in rural areas, and large enough to sell this story to a
3-year-old.
#+CAPTION: The Stork. Image by Soloneying, from https://commons.wikimedia.org/wiki/File:Ringed_white_stork.jpg
#+CAPTION: Downloaded Nov 22^{nd} 2019.
#+LABEL:fig:storksa
#+ATTR_LATEX: :width 5cm
[[file:Linear_Regression/Ringed_white_stork_2019-11-22_15-14-15.png]]


To bad, we are now grown up scientists with a penchant for critical
thinking. Rather than believing this story, we want to see the data, and ask
if this were true, we should see a good correlation between the number of storks
and the number of babies. Low and behold, these two variables, actually
correlate in a statistically significant way, i.e, the more storks we count in
a country, the higher the (human) birthrate. Since both variables increase
together, this is called a positive correlation. See Fig. [[fig:storks]]

#+CAPTION: The birthrate and the number of stork pairs correlate in a statistical significant way. 
#+CAPTION: This analysis suggest that each stork pair delivers about 29 human babies, and
#+CAPTION: that about 225 thousand babies were born otherwise. 
#+CAPTION: Data after cite:matthews-2000-stork-devil.
#+LABEL:fig:storks
#+ATTR_LATEX: :width 10cm
[[file:storks.png]]

Now, does this prove that the storks deliver the babies? Obviously (or so we
think) not. Just because two observable quantities correlate, does in no way
imply that one is the cause of the other. The more likely explanation is that
both variables are affected by a common process (i.e., industrialization).

It is a common mistake to confuse correlation with causation. Another
good example is to correlate drinking with heart attacks. This surely
will correlate but the story is more difficult. Are there e.g.,
patterns like drinkers tend to do less exercise than non-drinkers? So
even if you have a good hypothesis why two variables are correlated,
the correlation on its own, proves nothing.


** Understanding the results of a linear regression analysis
\index{linear regression!dependent variable}
\index{linear regression!independent variable}
Regression analysis compares how well a dataset of two variables (lets
call them =x= and =y=) can be described by a function which allows us
to predict the value of the dependent variable =y= based on the value
of the independent variable =x=.  In the case of a linear regression,
this can be expressed by a linear equation:
\begin{equation}
\label{eq:1}
y = a+mx
\end{equation}
where =a= denotes the y-axis intercept, =m= denotes the slope. Note
that the above equation is a simple model, which we can use to make
predictions about actual data. Linear regression analysis adjusts the
parameters =a= and =m= in such a way that the difference between the
measured data and the model prediction is minimized.

From a user perspective, we are interested to understand how good the
model actually is. and how to interpret the key indicators of a given
regression model:

 - r^2 :: or coefficient of determination. \index{linear
   regression!rsquare} \index{linear regression!coefficient of
   determination} This value is in the range from zero to one and
   expresses how much of the observed variance \index{linear
   regression!variance} in the data is explained by the regression
   model. So a value of 0.7 indicates that 70% of the variance is
   explained by the model, and that 30% of the variance is explained
   by other processes which are not captured by the linear model
   (e.g., measurements errors, or some non-linear effect affecting =x=
   and =y=). In Fig. [[fig:storks]] 38% of the variance in the birthrate
   can be explained by the increase in stork pairs.  Note that often
   you will also find the term R^2. For a simple linear regression with
   two variables, r^2 equals R^2. However, if your model incorporates
   more than 2 variables, these numbers can be different.
 - p :: When you do a linear regression, you basically state the
   hypothesis that =y= depends =x= and that they are linked by a
   linear equation. If you test a hypothesis, you however also have to
   test the so called *null-hypothesis*, which in this case would
   \index{linear regression!null hypothesis} state that =y= is
   \index{linear regression!p-value} unrelated to =x=. The p-value
   expresses the likelihood that the null-hypothesis is true. So a
   p-value of 0.1 indicates a 10% chance that your data does not
   correlate. A p-value of 0.01, indicates a 1% chance that your data
   is not correlated. Typically, we can reject the null-hypothesis if
   =p < 0.05=, in other words, we are 95% sure the null hypothesis is
   wrong. In Fig. [[fig:storks]], we are 99.2% sure the null hypothesis is
   wrong. Note that there is not always a simple relationship between
   r^2 and p.
 

** The statsmodel library
\index{library!statsmodel} \index{library!statsmodel!formula api}
Pythons success rests to a considerable degree on the myriad of third
party libraries which, unlike matlab, are typically free to use. In
the following we will use the "statsmodel" library, but there are
plenty of other statistical libraries we could use as well. 

The statsmodel library provides different interfaces. Here we will use
the formula interface which is similar to the R-formula
syntax. However not all statsmodel functions are available through
this interface (yet?). First we import the needed libraries:
#+BEGIN_SRC ipython
from typing import TypeVar # type hinting support
import os                  # os support
import pandas as pd        # use pandas to read the data
# and the statsmodel formula interface for the regression
import statsmodels.formula.api as smf
#+END_SRC

#+RESULTS:
: # Out [14]: 

Next we declare the non-standard type hints
#+BEGIN_SRC ipython
# declare the type hints
pdf = TypeVar('pandas.core.frame.DataFrame')
pds = TypeVar('pandas.core.series.Series')
smm = TypeVar('statsmodels.regression.linear_model.OLS')
smr = TypeVar('statsmodels.regression.linear_model.RegressionResultsWrapper')
npa = TypeVar('numpy.ndarray')
mpf = TypeVar('matplotlib.figure.Figure')
mpa = TypeVar('matplotlib.axes._subplots.AxesSubplot')
#+END_SRC

#+RESULTS:
: # Out [15]: 


Now we read the data:
#+BEGIN_SRC ipython
# the filename
fn :str = "storks_vs_birth_rate.csv" # file name

# read the data
if os.path.exists(fn): # check if the file is actually there
     df :pdf = pd.read_csv(fn)         # read data
     df.columns = ["Babies", "Storks"] # replace colum names
     print(df.head())
else:
     print("\n ------------------------------- \n")
     print(f"{fn} not found")
     print("\n ------------------------------- \n")
     exit()
#+END_SRC

#+RESULTS:
: # Out [16]: 
: # output
:    Babies  Storks
: 0      83     100
: 1      87     300
: 2     118       1
: 3     117    5000
: 4      59       9
: 

For the statistical analysis, we want to analyze whether the number of
storks predicts the number of babies. In other words does the birth
rate depend on the number of storks?
#+BEGIN_SRC ipython
# first we declare some variable names otherwise the below lines will
# look quite messy. Note that these variable are not initialized with
# any value. The next two lines are in fact non-statements and merely
# help to improve the clarifty of the code
model :smm      # this variable will hold our statistical model
results :smr    # this variable will hold the results of the analysis

# next initialize our statistical model which describes our analysis
# as well as the datasource. "ols" stands for ordinary least squares
model   = smf.ols(formula="Babies ~ Storks",data=df)
results = model.fit()      # fit the model to the data
print(results.summary())   # print the results of the analysis
#+END_SRC

#+RESULTS:
#+begin_example
# Out [17]: 
# output
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                 Babies   R-squared:                       0.385
Model:                            OLS   Adj. R-squared:                  0.344
Method:                 Least Squares   F-statistic:                     9.380
Date:                Fri, 17 Jan 2020   Prob (F-statistic):            0.00790
Time:                        15:04:14   Log-Likelihood:                -121.75
No. Observations:                  17   AIC:                             247.5
Df Residuals:                      15   BIC:                             249.2
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept    225.0287     93.561      2.405      0.030      25.609     424.448
Storks         0.0288      0.009      3.063      0.008       0.009       0.049
==============================================================================
Omnibus:                        3.469   Durbin-Watson:                   1.769
Prob(Omnibus):                  0.176   Jarque-Bera (JB):                2.486
Skew:                           0.925   Prob(JB):                        0.288
Kurtosis:                       2.707   Cond. No.                     1.16e+04
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 1.16e+04. This might indicate that there are
strong multicollinearity or other numerical problems.

#+end_example

Plenty of information here, probably more than the you asked for. But
note the first line, which states that 'Babies' is the dependent
variable. This is useful and will help you to catch errors in your
model definition. But what we really want are the slope, r^2 and
p-values. The below code demonstrates how to extract these from the
model results
#+BEGIN_SRC ipython  
# retrieve values from the model results
slope   :float = results.params[1]  # the slope
y0      :float = results.params[0]  # the y-intercept
rsquare :float = results.rsquared   # rsquare
pvalue  :float = results.pvalues[1] # the pvalue
#+END_SRC

#+RESULTS:
: # Out [18]: 





# bibliographystyle:gca
# bibliography:literatur/journals-new.bib,literatur/uli-with-students.bib,literatur/new.bib 


